{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Read data**"
      ],
      "metadata": {
        "id": "XqhWgALy-m9U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import pickle\n",
        "\n",
        "# Load DataFrame back from pickle file\n",
        "pkl_path = '/content/drive/MyDrive/PhD/Speech Synthesis/general_data.pkl'\n",
        "df = pd.read_pickle(pkl_path)\n",
        "\n",
        "df.info()"
      ],
      "metadata": {
        "id": "3d0TvtnA-Rq6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5e29d50-7f8b-4d01-e66e-f0e17f00e6f4"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 11221 entries, 0 to 11220\n",
            "Data columns (total 9 columns):\n",
            " #   Column            Non-Null Count  Dtype \n",
            "---  ------            --------------  ----- \n",
            " 0   Audio Files       11221 non-null  object\n",
            " 1   Text random       11221 non-null  object\n",
            " 2   Text keras        11221 non-null  object\n",
            " 3   Surprisal Values  11221 non-null  object\n",
            " 4   Speaker           11221 non-null  int64 \n",
            " 5   Emotion           11221 non-null  int64 \n",
            " 6   Target Sentence   11221 non-null  int64 \n",
            " 7   Fold              11221 non-null  int64 \n",
            " 8   Mel Spectrum      11221 non-null  object\n",
            "dtypes: int64(4), object(5)\n",
            "memory usage: 789.1+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the base path replacement function\n",
        "def replace_base_path(old_path):\n",
        "    new_base_path = '/content/drive/MyDrive/PhD/Speech Synthesis/data_mono'\n",
        "    # Extract the relevant part of the old path\n",
        "    relevant_part = old_path.replace('..\\\\podaci\\\\data_mono', '')\n",
        "    # Combine with the new base path\n",
        "    new_path = new_base_path + relevant_part.replace('\\\\', '/')\n",
        "    return new_path\n",
        "\n",
        "# Apply the function to the 'Audio Files' column\n",
        "df['Audio Files'] = df['Audio Files'].apply(replace_base_path)"
      ],
      "metadata": {
        "id": "vtRkTbz8NXMX"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mapping dictionary: map 10 folds to 5 folds\n",
        "fold_mapping = {\n",
        "    0: 0, 1: 0, 2: 1, 3: 1, 4: 2, 5: 2, 6: 3, 7: 3, 8: 4, 9: 4\n",
        "}\n",
        "\n",
        "# Apply the mapping to the Folds column\n",
        "df['Fold'] = df['Fold'].map(fold_mapping)"
      ],
      "metadata": {
        "id": "vorNrFORno4a"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 256\n",
        "vocab_size = 276\n",
        "\n",
        "k_test = 3\n",
        "k_val = 4 # k_test + 1\n",
        "train_data = df[df['Fold'] != k_test]\n",
        "train_data = train_data[train_data['Fold'] != k_val]\n",
        "test_data = df[df['Fold'] == k_test]\n",
        "val_data = df[df['Fold'] == k_val]"
      ],
      "metadata": {
        "id": "vEvO7LPDJWFW"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data preparation**"
      ],
      "metadata": {
        "id": "0-w38Fw5cYPp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        audio_file, _, text, surprisal_values, speaker, emotion, _, _, mel_targets = self.data.iloc[idx]\n",
        "        return torch.tensor(text), torch.tensor(surprisal_values), torch.tensor(mel_targets), idx"
      ],
      "metadata": {
        "id": "D58Ubnl0hY6N"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate your dataset\n",
        "train_dataset = CustomDataset(train_data)\n",
        "test_dataset = CustomDataset(test_data)\n",
        "val_dataset = CustomDataset(val_data)\n",
        "\n",
        "# Create a data loader\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "rC9WzUqJFCH3"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model**"
      ],
      "metadata": {
        "id": "IDgFOky_W8Td"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class TacotronWithSurprisal(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, encoder_hidden_dim, decoder_hidden_dim, mel_dim, batch_size):\n",
        "        super(TacotronWithSurprisal, self).__init__()\n",
        "\n",
        "        # Text embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        # Surprisal embedding layer\n",
        "        self.surprisal_embedding = nn.Linear(1, embedding_dim)\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.LSTM(embedding_dim, encoder_hidden_dim, batch_first=True)\n",
        "\n",
        "        # Attention mechanism\n",
        "        self.attention = nn.Linear(encoder_hidden_dim + decoder_hidden_dim, 1)\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.LSTMCell(embedding_dim + encoder_hidden_dim, decoder_hidden_dim)\n",
        "\n",
        "        # Post processing\n",
        "        #self.linear = nn.Linear(decoder_hidden_dim, 250)\n",
        "        self.fc1 = nn.Linear(13*256, 1024)  # (input_dimension, hidden_dim)\n",
        "        self.fc2 = nn.Linear(1024, 80*250)  # (hidden_dimension, output_dimension)\n",
        "\n",
        "        # Learnable initial decoder input\n",
        "        self.init_decoder_input = nn.Parameter(torch.zeros(embedding_dim))\n",
        "\n",
        "    def forward(self, text, surprisal_values):\n",
        "        # Embed text\n",
        "        text_embedded = self.embedding(text)\n",
        "        # Embed surprisal values\n",
        "        surprisal_embedded = self.surprisal_embedding(surprisal_values.unsqueeze(-1))\n",
        "        # Combine text and surprisal embeddings\n",
        "        combined_embedded = text_embedded #+ surprisal_embedded\n",
        "\n",
        "        # Encode the combined embeddings\n",
        "        encoder_outputs, (h_n, c_n) = self.encoder(combined_embedded)\n",
        "\n",
        "        # Initialize the decoder\n",
        "        batch_size = text.size(0)\n",
        "        seq_len = text.size(1)\n",
        "        decoder_hidden = (h_n[-1], c_n[-1])\n",
        "        decoder_input = self.init_decoder_input.expand(batch_size, -1)  # Initialize with learned parameter\n",
        "\n",
        "        # Prepare for attention mechanism\n",
        "        decoder_outputs = []\n",
        "        for t in range(seq_len):\n",
        "            # Compute attention weights\n",
        "            decoder_hidden_broadcasted = decoder_hidden[0].unsqueeze(1).expand(batch_size, seq_len, decoder_hidden[0].size(1))\n",
        "            attn_input = torch.cat((decoder_hidden_broadcasted, encoder_outputs), dim=2)\n",
        "            attn_weights = F.softmax(self.attention(attn_input), dim=1)\n",
        "            context_vector = torch.sum(attn_weights * encoder_outputs, dim=1)\n",
        "\n",
        "            # Decoder step\n",
        "            decoder_input_combined = torch.cat((decoder_input, context_vector), dim=-1)\n",
        "            decoder_hidden = self.decoder(decoder_input_combined, decoder_hidden)\n",
        "            decoder_outputs.append(decoder_hidden[0])\n",
        "\n",
        "            # Update decoder input for the next time step\n",
        "            decoder_input = self.init_decoder_input.expand(batch_size, -1)\n",
        "\n",
        "        # Post processing\n",
        "        # Stack sequences along a new axis\n",
        "        stacked_outputs = torch.stack(decoder_outputs, dim=1)\n",
        "        # Flatten the last two dimensions (13 and 256) into one\n",
        "        flattened_outputs = stacked_outputs.view(batch_size, -1)  # Shape: (batch_size, 13*256)\n",
        "        x = self.fc1(flattened_outputs)\n",
        "        x = torch.relu(x)\n",
        "        transformed_outputs = self.fc2(x)\n",
        "\n",
        "        # Final mel outputs\n",
        "        mel_outputs = transformed_outputs.view(batch_size, 80, 250)\n",
        "\n",
        "        return mel_outputs"
      ],
      "metadata": {
        "id": "0Z934k4d0VBl"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model instantiation example\n",
        "embedding_dim = 128\n",
        "encoder_hidden_dim = 256\n",
        "decoder_hidden_dim = 256\n",
        "mel_dim = 80\n",
        "\n",
        "model = TacotronWithSurprisal(vocab_size, embedding_dim, encoder_hidden_dim, decoder_hidden_dim, mel_dim, batch_size)\n",
        "\n",
        "# Example input data\n",
        "text = torch.randint(0, vocab_size, (batch_size, 13))  # Batch size 57, sequence length 13\n",
        "surprisal_values = torch.randn(batch_size, 13)\n",
        "\n",
        "# Forward pass\n",
        "mel_outputs = model(text, surprisal_values)\n",
        "print(mel_outputs.shape)  # Should be (batch_size, sequence_length, target_dim)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_TcN16av0FUg",
        "outputId": "73ffb79b-cb6b-42b4-cde5-f6a076fe9854"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([256, 80, 250])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Training**"
      ],
      "metadata": {
        "id": "J51VaGiCyUpl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "import os\n",
        "from tqdm import tqdm  # Import tqdm for the progress bar\n",
        "\n",
        "# Define loss and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Directory to save the best model\n",
        "save_dir = './best_model'\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "# Lists to store training and validation losses\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "# Example training loop\n",
        "num_epochs = 20\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "    # Use tqdm to create a progress bar for the training phase\n",
        "    with tqdm(total=len(train_dataloader), desc=f'Epoch {epoch+1}/{num_epochs}', unit='batch') as pbar:\n",
        "        for batch in train_dataloader:\n",
        "            text, surprisal_values, mel_targets, _ = batch\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            mel_outputs = model(text, surprisal_values)\n",
        "            # Compute loss\n",
        "            loss = criterion(mel_outputs, mel_targets)\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            # Update the progress bar\n",
        "            pbar.set_postfix({'train_loss': train_loss / (pbar.n + 1)})\n",
        "            pbar.update(1)\n",
        "\n",
        "    # Compute average training loss\n",
        "    train_loss /= len(train_dataloader)\n",
        "    train_losses.append(train_loss)  # Store the training loss\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_dataloader:\n",
        "            text, surprisal_values, mel_targets,_ = batch\n",
        "\n",
        "            # Forward pass\n",
        "            mel_outputs = model(text, surprisal_values)\n",
        "            # Compute loss\n",
        "            loss = criterion(mel_outputs, mel_targets)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    # Compute average validation loss\n",
        "    val_loss /= len(val_dataloader)\n",
        "    val_losses.append(val_loss)  # Store the validation loss\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs},\\n Training Loss: {train_loss}, Validation Loss: {val_loss}\")\n",
        "\n",
        "    # Save the model if the validation loss is the best we've seen so far\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_model_path = os.path.join(save_dir, f'best_model_{k_test}.pth')\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "        print('Model saved!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvFDTLkKf1fa",
        "outputId": "9c1e7061-153e-4b10-8577-5634e4a2c0a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20: 100%|██████████| 28/28 [00:47<00:00,  1.71s/batch, train_loss=1.16e+3]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20,\n",
            " Training Loss: 1157.977695465088, Validation Loss: 261.83404032389325\n",
            "Model saved!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/20: 100%|██████████| 28/28 [00:43<00:00,  1.54s/batch, train_loss=305]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/20,\n",
            " Training Loss: 305.3184574672154, Validation Loss: 241.85547892252603\n",
            "Model saved!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/20: 100%|██████████| 28/28 [00:43<00:00,  1.54s/batch, train_loss=291]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/20,\n",
            " Training Loss: 290.5182429722377, Validation Loss: 237.09769863552518\n",
            "Model saved!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/20: 100%|██████████| 28/28 [00:43<00:00,  1.54s/batch, train_loss=288]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/20,\n",
            " Training Loss: 288.1069112505232, Validation Loss: 234.51095072428384\n",
            "Model saved!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/20: 100%|██████████| 28/28 [00:44<00:00,  1.58s/batch, train_loss=289]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/20,\n",
            " Training Loss: 288.99495806012834, Validation Loss: 237.73765733506946\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/20: 100%|██████████| 28/28 [00:42<00:00,  1.53s/batch, train_loss=288]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/20,\n",
            " Training Loss: 288.3660109383719, Validation Loss: 239.0271470811632\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/20: 100%|██████████| 28/28 [00:42<00:00,  1.52s/batch, train_loss=294]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/20,\n",
            " Training Loss: 294.1887087140764, Validation Loss: 239.8666534423828\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/20: 100%|██████████| 28/28 [00:42<00:00,  1.53s/batch, train_loss=292]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/20,\n",
            " Training Loss: 291.88245064871654, Validation Loss: 237.8931393093533\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/20: 100%|██████████| 28/28 [00:44<00:00,  1.59s/batch, train_loss=290]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/20,\n",
            " Training Loss: 289.7557939801897, Validation Loss: 240.79495747884116\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/20: 100%|██████████| 28/28 [00:42<00:00,  1.52s/batch, train_loss=292]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/20,\n",
            " Training Loss: 292.39464569091797, Validation Loss: 244.68192206488715\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/20: 100%|██████████| 28/28 [00:43<00:00,  1.55s/batch, train_loss=290]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/20,\n",
            " Training Loss: 289.74111883980885, Validation Loss: 237.97555881076389\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/20: 100%|██████████| 28/28 [00:42<00:00,  1.53s/batch, train_loss=292]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/20,\n",
            " Training Loss: 291.85965510777066, Validation Loss: 250.54688686794705\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/20: 100%|██████████| 28/28 [00:43<00:00,  1.56s/batch, train_loss=290]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/20,\n",
            " Training Loss: 289.69002423967635, Validation Loss: 235.99182637532553\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/20: 100%|██████████| 28/28 [00:43<00:00,  1.55s/batch, train_loss=285]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/20,\n",
            " Training Loss: 284.8633826119559, Validation Loss: 244.55147806803384\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/20:  36%|███▌      | 10/28 [00:15<00:26,  1.49s/batch, train_loss=285]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot training and validation losses\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss Over Epochs')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mv4Wyv2DgD60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the best model\n",
        "best_model_path = os.path.join(save_dir, f'best_model_{k_test}.pth')\n",
        "model.load_state_dict(torch.load(best_model_path))\n",
        "model.eval()\n",
        "\n",
        "# Lists to store the test results\n",
        "audio_files = []\n",
        "speakers = []\n",
        "emotions = []\n",
        "model_outputs = []\n",
        "test_losses = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_dataloader:\n",
        "        text, surprisal_values, mel_targets, indices = batch\n",
        "\n",
        "        # Forward pass\n",
        "        mel_outputs = model(text, surprisal_values)\n",
        "\n",
        "        # Compute loss for each example in the batch\n",
        "        for i in range(text.size(0)):\n",
        "            single_mel_output = mel_outputs[i].unsqueeze(0)\n",
        "            single_mel_target = mel_targets[i].unsqueeze(0)\n",
        "            loss = criterion(single_mel_output, single_mel_target)\n",
        "\n",
        "            # Collect the results\n",
        "            model_outputs.append(single_mel_output.cpu().numpy())\n",
        "            test_losses.append(loss.item())\n",
        "\n",
        "        # Collect corresponding data info\n",
        "        indices = indices.numpy()  # Convert indices to numpy array\n",
        "        audio_files.extend(test_data.iloc[indices]['Audio Files'].tolist())\n",
        "        speakers.extend(test_data.iloc[indices]['Speaker'].tolist())\n",
        "        emotions.extend(test_data.iloc[indices]['Emotion'].tolist())\n",
        "\n",
        "# Compute average test loss\n",
        "test_loss = sum(test_losses) / len(test_losses)\n",
        "print(f\"Test Loss (with best model): {test_loss}\")\n",
        "\n",
        "# Combine the results into a DataFrame\n",
        "results_df = pd.DataFrame({\n",
        "    'Audio Files': audio_files,\n",
        "    'Speaker': speakers,\n",
        "    'Emotion': emotions,\n",
        "    'Test Loss': test_losses\n",
        "})\n",
        "\n",
        "# Save results to a CSV file\n",
        "results_csv_path = f'/content/drive/MyDrive/PhD/Speech Synthesis/Results/results_{k_test}.csv'\n",
        "results_df.to_csv(results_csv_path, index=False)\n",
        "print(f\"Test results saved to {results_csv_path}\")\n"
      ],
      "metadata": {
        "id": "ysBux0evjB1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_df"
      ],
      "metadata": {
        "id": "AKUCa-IgiOtO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}